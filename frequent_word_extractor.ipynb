{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module for extracting text from non-text files\n",
    "import textract\n",
    "# import modules for handling language data\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function for converting a selected PDF file to text\n",
    "def read_file(filepath):\n",
    "    # use tesseract to read PDF files\n",
    "    encoded_text = textract.process(filepath, method='tesseract', encoding='utf-8')\n",
    "    text = encoded_text.decode('utf-8')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to parse text data, common_notes is a user defined list that contains commonly found\n",
    "# trivial words/signs that need to be removed \n",
    "def get_words(text, common_notes = []):\n",
    "    # tokenize text data\n",
    "    tokens = word_tokenize(text)\n",
    "    # define punctuations that need to be removed from tokenized data\n",
    "    punctuations = ['(', ')', ';', ':', '[', ']', ',', '-', '.', \"â€™\"]\n",
    "    # define common english stop words that need to be removed from tokenized data\n",
    "    stop_words = stopwords.words('english')\n",
    "    # remove puncuations, stop words, user-defined common_notes and integers\n",
    "    keywords = [word.lower() for word in tokens if word.lower() not in stop_words and word not in punctuations \n",
    "                and word.lower() not in common_notes and not str.isdigit(word)]\n",
    "    # create a stemmer object and lemmatize words\n",
    "    wnl = WordNetLemmatizer()\n",
    "    stem_keywords = [wnl.lemmatize(word) for word in keywords]\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file path, common_notes and call functions\n",
    "filepath = 'GRE_Text_Completion.pdf'\n",
    "\n",
    "common_notes = ['a.', 'b.', 'c.', 'd.', 'e.', 'f.', 'g.', 'h.', 'i.', 'i', 'ii', 'iii']\n",
    "\n",
    "text = read_file(filepath)\n",
    "keywords = get_words(text, common_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of found keywords\n",
    "words_df = pd.DataFrame(keywords, columns = ['Keyword'])\n",
    "# count the frequency of each keyword, store in a new dataframe\n",
    "words_frequency = pd.DataFrame(words_df['Keyword'].value_counts())\n",
    "words_frequency.rename({'Keyword':'Frequency'}, axis = 1, inplace  = True)\n",
    "# export to excel\n",
    "words_frequency.to_excel('Frequent_Words.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hla</th>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grew</th>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>|</th>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section</th>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&gt;</th>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>many</th>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>even</th>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>often</th>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>may</th>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hard</th>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>although</th>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>easy</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientific</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>___</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Frequency\n",
       "hla               245\n",
       "grew              245\n",
       "|                 244\n",
       "section           241\n",
       ">                 137\n",
       "one               134\n",
       "many              131\n",
       "even              115\n",
       "often             102\n",
       "new                91\n",
       "may                91\n",
       "people             86\n",
       "median             80\n",
       "hard               79\n",
       "although           74\n",
       "easy               68\n",
       "would              67\n",
       "work               65\n",
       "scientific         65\n",
       "book               64\n",
       "science            62\n",
       "public             58\n",
       "___                58\n",
       "time               57\n",
       "research           57"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_frequency.head(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
