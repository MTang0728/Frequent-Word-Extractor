{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module for extracting text from non-text files\n",
    "import textract\n",
    "# import modules for handling language data\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function for converting a selected PDF file to text\n",
    "def read_file(filepath):\n",
    "    # use tesseract to read PDF files\n",
    "    encoded_text = textract.process(filepath, method='tesseract', encoding='utf-8')\n",
    "    text = encoded_text.decode('utf-8')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to parse text data, common_notes is a user defined list that contains commonly found\n",
    "# trivial words/signs that need to be removed \n",
    "def get_words(text, common_notes = []):\n",
    "    # tokenize text data\n",
    "    tokens = word_tokenize(text)\n",
    "    # define punctuations that need to be removed from tokenized data\n",
    "    punctuations = ['(', ')', ';', ':', '[', ']', ',', '-', '.', \"â€™\"]\n",
    "    # define common english stop words that need to be removed from tokenized data\n",
    "    stop_words = stopwords.words('english')\n",
    "    # remove puncuations, stop words, user-defined common_notes and integers\n",
    "    keywords = [word.lower() for word in tokens if word.lower() not in stop_words and word not in punctuations \n",
    "                and word.lower() not in common_notes and not str.isdigit(word)]\n",
    "    # create a stemmer object and lemmatize words\n",
    "    wnl = WordNetLemmatizer()\n",
    "    stem_keywords = [wnl.lemmatize(word) for word in keywords]\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file path, common_notes and call functions\n",
    "filepath = 'GRE_sample.pdf'\n",
    "\n",
    "common_notes = ['a.', 'b.', 'c.', 'd.', 'e.', 'f.', 'g.', 'h.', 'i.', 'i', 'ii', 'iii']\n",
    "\n",
    "text = read_file(filepath)\n",
    "keywords = get_words(text, common_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of found keywords\n",
    "words_df = pd.DataFrame(keywords, columns = ['Keyword'])\n",
    "# count the frequency of each keyword, store in a new dataframe\n",
    "words_frequency = pd.DataFrame(words_df['Keyword'].value_counts())\n",
    "words_frequency.rename({'Keyword':'Frequency'}, axis = 1, inplace  = True)\n",
    "# export to excel\n",
    "words_frequency.to_excel('Frequent_Words.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>whose</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>may</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>useful</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>art</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>since</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>readers</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&gt;</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texts</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>books</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l241</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>easy</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>easily</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>|</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laboratory</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gren</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dieticians</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensure</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collective</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>illuminating</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manageable</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>safeguard</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contain</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exhaustiveness</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Frequency\n",
       "whose                   2\n",
       "may                     2\n",
       "useful                  2\n",
       "art                     2\n",
       "since                   2\n",
       "readers                 2\n",
       ">                       2\n",
       "texts                   2\n",
       "books                   2\n",
       "l241                    2\n",
       "easy                    2\n",
       "easily                  2\n",
       "|                       2\n",
       "author                  2\n",
       "laboratory              2\n",
       "gren                    2\n",
       "dieticians              2\n",
       "come                    2\n",
       "ensure                  1\n",
       "collective              1\n",
       "illuminating            1\n",
       "manageable              1\n",
       "safeguard               1\n",
       "contain                 1\n",
       "exhaustiveness          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_frequency.head(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
